Workflow
======================================================

In this introductory tutorial, we go through the different steps of a complete Matilda's workflow.

.. image:: main.jpg
   :alt: The framework of Matilda
   :scale: 25%
   :align: center

1. Data Preperation  
--------------------------------------
Matilda could handle the sequencing data of RNA, ADT and ATAC. It expects raw count data for RNA and ADT modalities and the 'gene activity score' generated by Seurat from raw count data. Before modelling, you could consider remove part of the Genes with fewer than x% quantifications across cells in each of the three modalities. The 'x' is based on your criteria. Specific cell types could also be considered removing if they are extremely less or not important. 

2. Data Simulation and aggregation 
-----------------------------------------------
The first process of Matilda is the data simulation. As mentioned in the :doc:`DESIGN` and shown as the figure above, it is operated by the VAE in model. 

The encoders in the VAE component are shareable for both data simulation and classification tasks, and consist of one learnable point wise parameter layer and one fully-connected layer to the input layer. Because ADT modality has significantly fewer features than RNA and ATAC modalities, we set empirically, based on model selection, the numbers of neurons for encoders of RNA, ADT, and ATAC modalities to be 185, 30, and 185, respectively. To learn a latent space that integrates the information from across modalities, we concatenated the output from the encoder trained from each data modality to perform joint learning using a fully-connected layer with 100 neurons, followed by a VAE reparameterization process (11). Next, the fully-connected layer of the latent space is split into two branches with one branch fed into the decoders and the other branch fed into the fully connected classification network. For the decoder branch, it consists of multiple decoders each corresponds to an input data modality. Each decoder consists of one fully-connected layer to the output layer that has the same number of neurons as the features in the corresponding data modality. 

We applied Matilda to five recent multimodal single-cell omics datasets including a TEA-seq dataset that profiles RNA, ADT and ATAC modalities in human PBMC samples; three CITE-seq datasets that profile RNA and ADT modalities in human PBMC samples; and a SHARE-seq dataset that profiles RNA and ATAC modalities in mouse skin samples (Supplementary Figure S1). To test if Matilda is able to simulate multimodal omics data in a cell-type specific manner, we first visualized cells using each modality on UMAPs (Figure 1B and Supplementary Figure S2) and highlighted cells from representative cell types using real and Matilda simulated data. We found that Matilda not only precisely simulates each data modality in a cell type-specific manner but also denoizes the outliers in the real data, (e.g. ADT modality of B cells and CD14 cells in CITE-seq data; Supplementary Figure S2a). 

To further assess the performance of Matilda on data simulation, we compared the correlation structure of highly variable genes (HVGs) by each data modality using real data and those simulated by Matilda (Figure 2A–C and Supplementary Figure S3). We found that data simulated by Matilda closely resemble the correlation structure of real data across all modalities. While no other methods are currently available for simulating multimodal single-cell omics data besides Matilda, various methods have been devel oped for simulating from scRNA-seq data (3). We, therefore, compared the simulation results of Matilda on RNA modality with those generated from scGAN (30), a simulation method based on deep generative adversarial networks, ACTIVA(31), a deep learning method based on adversarial VAE, and Sparsim (17), one of the best performing simulation methods based on mixture modelling (3). We found that in most cases data simulated from Matilda for the RNA modality better preserve the correlation structure in the real data compared to alternative methods as quantified in Figure 2D–F. These results demonstrate the ability of Matilda on simulating multiple data modalities in a cell-type-specific manner in multimodal single-cell omics datasets.


3. Data augmentaion and balancing
--------------------------------------
During the model training process, Matilda performs data augmentation and balancing using simulated data from the VAE component. Specifically, Matilda first ranks the cell types in the training dataset by the number of cells in each type. The cell type corresponding to the median number is used as the reference and those that have smaller numbers of cells are augmented to have the same number of cells as the median using VAE simulated single-cell multimodal data for each cell type. Cell types that have larger numbers of cells than the median number are randomly down-sampled to match the median number of cells as well. This strategy helps Matilda to mitigate imbalanced cell type distribution in the data (21) and better learn the molecular features of under-represented and rare cell types.

4. Dimension Reduction
--------------------------------------
During model training, Matilda learns to combine and reduce the feature dimensions of multimodal single-cell omics data to a latent space using its VAE component in the frame work (Figure 1A). The trained VAE of Matilda thus can be used for multimodal feature integration and dimension re duction of both the training and new data. Several alternative methods are available for such tasks. These include Seu rat (14) and totalVI (4), which are designed for integrating RNA and ADT modalities in CITE-seq data; Conos (32) and multiVI (33), which are designed for integrating RNA and ATAC modalities such as these in SHARE-seq data; and Multigrate (34), which is not limited to specific paired assays and can be applied to both bi- and tri-modality data. Comparing to these methods, we found that the dimension reduced data from Matilda shows significantly better cell type separation under UMAP projection (Figure 3A, B). To further quantify these visual observations, we clustered the dimension reduced data generated from each method using a simple k-means clustering algorithm and analysed the concordance of the clustering output with the cell type labels provided from their original studies using a panel of concordance metrics including ARI, NMI, FM, and Jaccard index (see Materials and Methods). We found that in most cases Matilda generated dimension reduced datasets led to higher clustering concordance with respect to the original cell type labels across all datasets irrespective of the metrics (Figure 3C–E and Supplementary Figure S4). These results demonstrate the superior performance of Matilda for integrating and reducing feature dimensions in multimodal single-cell omics data and its utility for subsequent applications such as data visualization and clustering of cell types

5. Classification
--------------------------------------
To evaluate Matilda on cell type classification using multimodal single-cell omics data, we performed both five-fold cross-validation (repeated 5 times) and training and test using different batches within each dataset (Supplementary Figure S1b). While several methods have been developed recently for transferring cell type labels across different data modalities for multimodal single-cell omics data (37–39), there are currently few methods specifically designed for cell type classification by using all data modalities from such data. To this end, we resorted to comparing methods that are developed for cell type classification from scRNA-seq data by using RNA modality only (40) and UMINT (31), a method designed for integrating multiple data modalities to low-dimensional embeddings which can be used for cell type classification. We found that Matilda classifies cells significantly more accurately across all datasets under both the cross-validation settings (Figure 4A) and those from training and test using different batches within each dataset (Figure 4B) than other state-of-the-art cell type classification methods that use only RNA modality or those from using integrated embeddings generated by UMINT. The breakdown of the classification results from training and test using each pair of data batches reveals that Matilda led to higher cell type classification accuracy across all pairs in all four datasets that contain multiple data batches (Figure 4C). 
To test if the performance of Matilda is impacted by the reduced size of the training data, we performed a stratified sampling of each cell type from CITE-seq and TEA-seq datasets generated by Ramaswamy et al. (13) and Swan son et al. (12), respectively, 80%, 50% and 20% of cells and trained each classification model using these subsampled datasets. We found that the performance of Matilda is largely maintained even when the model was trained on a small proportion of cells from the original datasets (Supplementary Figure S5). It is worth noting that the improved cell type classification accuracy of Matilda is not a sacrifice in speed on model training or classification of test data (Supplementary Figure S6). Since Matilda uses multi task learning and the simulated data from the VAE com ponent for data augmentation, we also evaluated the impact of these procedures on cell type classification accuracy. We found that, across all five datasets, multi-task learning indeed improved cell type classification than learning each task independently (Supplementary Figure S7a), and data augmentation resulted in better performance than those without (Supplementary Figure S7b). Together, these results demonstrate the utility of multi-task learning and data augmentation from simulation for improving cell type classification and highlight Matilda’s increased cell type classification accuracy using multimodalities compared to alternative methods that use only RNA modality.

6. Feature selection 
--------------------------------------
Finally, the neural network trained for cell type classification in Matilda can be used for multimodal feature selection using methods such as integrated gradient (IG) descent (22) and saliency procedures (23), and thus can lead to the selection of cell-type-specific features across all available modalities in the datasets. Figure 5A, B visualize top-ranked features selected by Matilda using IG for CD14 monocytes and Naive B cells, respectively, in each data modality in the TEA-seq dataset. The RNA and ADT expression levels and the ATAC activity of selected genes across all cell types in the dataset are shown in Figure 5C, D. As expected, these analyses reveal that features selected by Matilda for each data modality show expression specificity towards their respective cell types, demonstrating their potential usage for characterizing cell identity and their underlying molecular programs.
To evaluate the top features selected by Matilda across multiple data modalities and those selected from RNA modality using popular methods such as t-test and limma (7), and those specifically designed for scRNA-seq (e.g. MAST(8), ROC), and recently proposed deep learning feature selection methods, including PROPOSE (35) and scCapsNet (41), we compared their utility in classifying each cell type in each dataset. We found that cell-type-specific features selected by Matilda from multiple data modalities on average resulted in more accurate discrimination of their respective cell types as shown by the scatter plot and the overall rankings of methods in each dataset (Figure 5Eand Supplementary Figure S8). Within the two feature selection methods implemented in Matilda, IG appears to perform slightly better than saliency and is hence the recommended approach in Matilda for feature selection from multimodal single-cell omics data. Together, these results demonstrate Matilda as a useful approach for feature selection from multiple data modalities for cell type characterization and other downstream analyses.


7. Training continue
-----------------------------


The multi-task neural networks in Matilda consist of multimodality-specific en coders and decoders in a variational autoencoder (VAE) componentfordatasimulationandafully-connectedclassi f ication network for cell type classification. The encoders in the VAEcomponent are shareable for both data simulation and classification tasks, and consist of one learnable point wise parameter layer and one fully-connected layer to the input layer. Because ADT modality has significantly fewer features than RNA and ATAC modalities, we set empiri cally, based on model selection, the numbers of neurons for encoders of RNA, ADT, and ATAC modalities to be 185, 30, and 185, respectively. To learn a latent space that inte grates the information from across modalities, we concate nated the output from the encoder trained from each data modality to perform joint learning using a fully-connected layer with 100 neurons, followed by a VAE reparameteri zation process (11). Next, the fully-connected layer of the latent space is split into two branches with one branch fed into the decoders and the other branch fed into the fully connectedclassification network.Forthedecoderbranch,it consists of multiple decoders each corresponds to an input datamodality.Eachdecoderconsistsofonefully-connected layer to the output layer that has the same number of neu rons as the features in the corresponding data modality. For each fully-connected layer in the VAE component, batch normalization (18), shortcut (19) were utilized in the model. ReLU activation was used in all fully-connected layers ex cept in the reparameterization process. Dropout (r = 0.2) was utilized only for fully-connected layers in encoders. For the classification branch, it consists of the latent space as input to a fully-connected layer with a dimension equal to the number of cell types in the training data. The fully connected layer outputs a probability vector for cell type prediction through a SoftMax function.







